# ğŸ‡¦ğŸ‡º Australia Company ETL Pipeline

> **Entity Resolution Pipeline**: Common Crawl + Australian Business Register (ABR) + LLM-Enhanced Matching

## ğŸ“‹ Overview

### Problem Statement

This pipeline solves the challenge of matching company information from:
- **Common Crawl** (March 2025 Index) - Extracting ~200,000 Australian company websites
- **Australian Business Register (ABR)** - Official government registry with ~3M businesses

### What This Pipeline Does

| Step | Description | Output |
|------|-------------|--------|
| **1. Extract** | Download and parse WET files from Common Crawl; Parse ABR XML bulk extracts | Raw company data |
| **2. Transform** | Clean names, normalize text, standardize addresses | Cleaned datasets |
| **3. Match** | Fuzzy matching + LLM verification for entity resolution | Match pairs with scores |
| **4. Load** | Upsert to PostgreSQL with unified golden records | Production database |

### Key Features

âœ… **Hybrid Entity Matching** - Combines fuzzy string matching with LLM semantic understanding  
âœ… **Blocking Strategy** - Reduces comparison space from billions to thousands  
âœ… **ABN Validation** - Validates Australian Business Numbers using official checksum  
âœ… **Scalable** - Supports PySpark for distributed processing  
âœ… **Docker Ready** - One-command deployment with PostgreSQL  

---

## ğŸš€ Quick Start

### Large-Scale Processing (TB-scale CommonCrawl)

For processing large datasets (100K+ records):

```bash
# Install dependencies
pip install pyspark rapidfuzz

# Large dataset with specific number of workers
python src/pipeline.py --workers 16 --max-records 1000000

# Use existing downloaded files (skip download)
python src/pipeline.py --skip-download --workers 8
```

**CLI Options:**

| Option | Description | Default |
|--------|-------------|---------|
| `--workers` | Number of parallel workers | Auto-detected |
| `--max-records` | Maximum records to process | All |
| `--skip-download` | Use existing downloaded files | `False` |
| `--skip-load` | Skip loading to database | `False` |
| `--llm` | Enable LLM-based matching | `False` |
| `--config` | Path to configuration file | `config/pipeline_config.yaml` |

### Option 4: Using Make (Linux/Mac)

```bash
make install      # Install dependencies
make run          # Run pipeline
make test         # Run tests
make docker-up    # Start Docker services
```

---

## ğŸ­ Production Deployment

### Prerequisites

Before running in production, ensure you have:

- **Python 3.10+** installed
- **Docker & Docker Compose** installed
- **OpenAI API Key** (for LLM matching)
- **50GB+ disk space** for data files
- **16GB+ RAM** recommended for large datasets

### Step 1: Environment Setup

```bash
# Clone repository
git clone https://github.com/medhaparwe/australia-company-etl-pipeline
cd australia-company-etl-pipeline

# Create and activate virtual environment
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Install all dependencies
pip install -r requirements.txt

# Create data directories
mkdir -p data/raw/commoncrawl data/raw/abr data/processed data/output logs
```

### Step 3: Start Database

```bash
# Start PostgreSQL with Docker
docker-compose up -d postgres

# Wait for database to be ready (check logs)
docker-compose logs -f postgres

# Verify connection
docker-compose exec postgres psql -U postgres -d companydb -c "SELECT version();"

# Create tables and indexes
docker-compose exec postgres psql -U postgres -d companydb -f /docker-entrypoint-initdb.d/init.sql
```

### Step 4: Run Full Production Pipeline

```bash
# Run full ETL pipeline with all data
python src/pipeline.py  --config config/pipeline_config.yaml --max-records 200000 --llm

# Monitor progress in real-time
tail -f logs/pipeline.log
```

**Expected Production Output:**
```
2025-11-29 10:00:00 - pipeline - INFO - Starting pipeline run: prod-abc123
==================================================
STEP 1: EXTRACT
==================================================
2025-11-29 10:00:05 - INFO - Parsing 50 WET files...
2025-11-29 10:15:00 - INFO - Extracted 200,000 Common Crawl records
2025-11-29 10:15:30 - INFO - Parsing ABR XML bulk extract...
2025-11-29 10:25:00 - INFO - Extracted 3,000,000 ABR records
==================================================
STEP 2: TRANSFORM
==================================================
2025-11-29 10:25:05 - INFO - Cleaning Common Crawl data...
2025-11-29 10:30:00 - INFO - Cleaned CC data: 185,000 records
2025-11-29 10:30:05 - INFO - Cleaning ABR data...
2025-11-29 10:40:00 - INFO - Cleaned ABR data: 2,850,000 records
==================================================
STEP 3: ENTITY MATCHING
==================================================
2025-11-29 10:40:05 - INFO - Generating blocking keys...
2025-11-29 10:42:00 - INFO - Matching 185,000 CC with 2,850,000 ABR records
2025-11-29 11:30:00 - INFO - Fuzzy matching complete: 160,000 candidates
2025-11-29 11:30:05 - INFO - LLM verification for 8,000 edge cases...
2025-11-29 11:45:00 - INFO - LLM verification complete
2025-11-29 11:45:01 - INFO - Found 152,000 matches
2025-11-29 11:45:01 - INFO - Average match score: 86.50%
==================================================
STEP 4: LOAD
==================================================
2025-11-29 11:45:05 - INFO - Loading to PostgreSQL...
2025-11-29 11:55:00 - INFO - Loaded 185,000 web_companies
2025-11-29 12:15:00 - INFO - Loaded 2,850,000 abr_entities
2025-11-29 12:20:00 - INFO - Loaded 152,000 match_results
2025-11-29 12:25:00 - INFO - Created 152,000 unified_companies
==================================================
PIPELINE COMPLETED SUCCESSFULLY
==================================================
Duration: 2 hours 25 minutes
Total matches: 152,000
Match rate: 82.2%
High confidence matches: 128,000 (84.2%)
LLM verified: 8,000 (5.3%)
==================================================
```

### Step 5: Run dbt Transformations

```bash
# Navigate to dbt directory
cd dbt

# Install dbt dependencies
dbt deps

# Test database connection
dbt debug

# Run all dbt models
dbt run

# Run data quality tests
dbt test

# Generate and view documentation
dbt docs generate
dbt docs serve --port 8080
```

**Expected dbt Output:**
```
Running with dbt=1.7.0
Found 5 models, 12 tests, 3 sources

Concurrency: 4 threads (target='prod')

1 of 5 START sql view model staging.stg_web_companies .................. [RUN]
2 of 5 START sql view model staging.stg_abr_entities ................... [RUN]
1 of 5 OK created sql view model staging.stg_web_companies ............. [OK in 2.34s]
2 of 5 OK created sql view model staging.stg_abr_entities .............. [OK in 2.56s]
3 of 5 START sql table model intermediate.int_matched_companies ........ [RUN]
3 of 5 OK created sql table model intermediate.int_matched_companies ... [OK in 45.23s]
4 of 5 START sql table model marts.dim_companies ....................... [RUN]
5 of 5 START sql table model marts.fct_match_statistics ................ [RUN]
4 of 5 OK created sql table model marts.dim_companies .................. [OK in 32.45s]
5 of 5 OK created sql table model marts.fct_match_statistics ........... [OK in 5.67s]

Completed successfully

Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
```

### Step 6: Verify Results

```bash
# Connect to database
docker-compose exec postgres psql -U postgres -d companydb

# Check record counts
SELECT 'web_companies' as table_name, COUNT(*) as count FROM web_companies
UNION ALL
SELECT 'abr_entities', COUNT(*) FROM abr_entities
UNION ALL
SELECT 'entity_match_results', COUNT(*) FROM entity_match_results
UNION ALL
SELECT 'unified_companies', COUNT(*) FROM unified_companies;

# View match statistics
SELECT * FROM fct_match_statistics;

# View sample matched companies
SELECT 
    abn,
    canonical_name,
    trading_name,
    website_url,
    state,
    confidence_score,
    data_source
FROM dim_companies 
WHERE data_source = 'MERGED'
ORDER BY confidence_score DESC
LIMIT 20;

# Check match distribution by state
SELECT 
    state, 
    COUNT(*) as match_count,
    ROUND(AVG(confidence_score)::numeric, 3) as avg_score
FROM dim_companies
WHERE state IS NOT NULL
GROUP BY state
ORDER BY match_count DESC;

# Exit psql
\q
```

### Step 7: Export Results

```bash
# Export unified companies to CSV
docker-compose exec postgres psql -U postgres -d companydb -c \
    "COPY (SELECT * FROM dim_companies) TO STDOUT WITH CSV HEADER" \
    > data/output/unified_companies.csv

# Export match statistics to JSON
docker-compose exec postgres psql -U postgres -d companydb -c \
    "SELECT row_to_json(t) FROM fct_match_statistics t" \
    > data/output/match_statistics.json

# Generate Parquet files for analytics
python -c "
import pandas as pd
from sqlalchemy import create_engine

engine = create_engine('postgresql://postgres:postgres123@localhost:5432/companydb')

# Export to Parquet
pd.read_sql('SELECT * FROM dim_companies', engine).to_parquet('data/output/dim_companies.parquet')
pd.read_sql('SELECT * FROM entity_match_results', engine).to_parquet('data/output/match_results.parquet')
print('Exported to Parquet files')
"
```

### Production Commands Summary

```bash
# Complete production workflow in one script
#!/bin/bash
set -e

echo "=== PRODUCTION PIPELINE ==="

# 1. Setup
source venv/bin/activate
source .env

# 2. Start database
docker-compose up -d postgres
sleep 10

# 3. Run ETL pipeline
python src/pipeline.py --max-records 200000 --llm

# 4. Run dbt transformations
cd dbt && dbt run && dbt test && cd ..

# 5. Generate statistics
docker-compose exec postgres psql -U postgres -d companydb -c \
    "SELECT * FROM fct_match_statistics;"

# 6. Export results
python -c "
import pandas as pd
from sqlalchemy import create_engine
engine = create_engine('postgresql://postgres:postgres123@localhost:5432/companydb')
df = pd.read_sql('SELECT * FROM dim_companies', engine)
df.to_csv('data/output/unified_companies.csv', index=False)
print(f'Exported {len(df)} companies')
"

echo "=== PIPELINE COMPLETE ==="
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA SOURCES                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Common Crawl (WET)         â”‚        ABR XML Bulk Extract         â”‚
â”‚     ~200k Australian Websites     â”‚      ~3M Business Registrations     â”‚
â”‚  https://commoncrawl.org/         â”‚      https://data.gov.au/           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                    â”‚
                  â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXTRACTION LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   parse_commoncrawl.py    â”‚      â”‚        parse_abr.py           â”‚   â”‚
â”‚  â”‚   â€¢ Stream WET.gz files   â”‚      â”‚   â€¢ Parse XML with iterparse  â”‚   â”‚
â”‚  â”‚   â€¢ Filter .au domains    â”‚      â”‚   â€¢ Extract ABN, Name, State  â”‚   â”‚
â”‚  â”‚   â€¢ Extract URL + Text    â”‚      â”‚   â€¢ Handle large files        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                    â”‚
                  â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       TRANSFORMATION LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   clean_commoncrawl.py    â”‚      â”‚        clean_abr.py           â”‚   â”‚
â”‚  â”‚   â€¢ Normalize names       â”‚      â”‚   â€¢ Validate ABN checksum     â”‚   â”‚
â”‚  â”‚   â€¢ Remove PTY/LTD/etc    â”‚      â”‚   â€¢ Standardize state codes   â”‚   â”‚
â”‚  â”‚   â€¢ Extract domain        â”‚      â”‚   â€¢ Handle null values        â”‚   â”‚
â”‚  â”‚   â€¢ Generate block_key    â”‚      â”‚   â€¢ Generate block_key        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ENTITY MATCHING LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      entity_match.py                               â”‚  â”‚
â”‚  â”‚                                                                    â”‚  â”‚
â”‚  â”‚   Step 1: BLOCKING                                                 â”‚  â”‚
â”‚  â”‚   â””â”€â”€ Group by first 4 chars of normalized name                   â”‚  â”‚
â”‚  â”‚   â””â”€â”€ Reduces: 200K Ã— 3M â†’ ~10K pairs per block                   â”‚  â”‚
â”‚  â”‚                                                                    â”‚  â”‚
â”‚  â”‚   Step 2: FUZZY MATCHING                                           â”‚  â”‚
â”‚  â”‚   â””â”€â”€ RapidFuzz token_sort_ratio                                  â”‚  â”‚
â”‚  â”‚   â””â”€â”€ Score range: 0.0 - 1.0                                       â”‚  â”‚
â”‚  â”‚                                                                    â”‚  â”‚
â”‚  â”‚   Step 3: LLM VERIFICATION (Optional, for scores 0.6-0.85)        â”‚  â”‚
â”‚  â”‚   â””â”€â”€ GPT-4o-mini semantic comparison                              â”‚  â”‚
â”‚  â”‚   â””â”€â”€ Returns: {"match": true, "score": 0.92, "reason": "..."}    â”‚  â”‚
â”‚  â”‚                                                                    â”‚  â”‚
â”‚  â”‚   FINAL SCORE = 0.7 Ã— fuzzy_score + 0.3 Ã— llm_score               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            LOAD LAYER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                     load_postgres.py                               â”‚  â”‚
â”‚  â”‚   â€¢ Create tables with proper indexes                              â”‚  â”‚
â”‚  â”‚   â€¢ Batch insert with ON CONFLICT upsert                          â”‚  â”‚
â”‚  â”‚   â€¢ Generate unified_companies golden records                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PostgreSQL                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ web_companies  â”‚  â”‚  abr_entities  â”‚  â”‚  entity_match_results      â”‚ â”‚
â”‚  â”‚ (from CC)      â”‚  â”‚  (from ABR)    â”‚  â”‚  (match pairs + scores)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚  unified_companies  â”‚  â† Golden Record             â”‚
â”‚                    â”‚  (merged records)   â”‚                              â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
australia-company-etl/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                  # This documentation
â”œâ”€â”€ ğŸ“„ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ docker-compose.yml         # Docker services (PostgreSQL, pgAdmin)
â”œâ”€â”€ ğŸ“„ Dockerfile                 # Container build instructions
â”œâ”€â”€ ğŸ“„ .gitignore                 # Git ignore patterns
â”‚
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ pipeline_config.yaml      # Main configuration
â”‚   â””â”€â”€ logging.conf              # Logging settings
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py               # ğŸ¯ Main orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ common/                # Shared utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ utils.py              # Text normalization, ABN validation
â”‚   â”‚   â”œâ”€â”€ spark_session.py      # PySpark session management
â”‚   â”‚   â””â”€â”€ llm_matcher.py        # OpenAI GPT integration
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ ingest/                # Data extraction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ download_commoncrawl.py   # Download WET files
â”‚   â”‚   â”œâ”€â”€ parse_commoncrawl.py      # Parse WET to extract companies
â”‚   â”‚   â”œâ”€â”€ download_abr.py           # Download ABR XML
â”‚   â”‚   â””â”€â”€ parse_abr.py              # Parse ABR XML
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ transform/             # Data transformation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ clean_commoncrawl.py      # Clean CC data
â”‚   â”‚   â”œâ”€â”€ clean_abr.py              # Clean ABR data
â”‚   â”‚   â”œâ”€â”€ entity_match.py           # ğŸ¯ Entity matching logic
â”‚   â”‚   â””â”€â”€ feature_engineering.py    # Match features
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ load/                  # Database loading
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ create_tables.sql         # PostgreSQL schema
â”‚       â”œâ”€â”€ load_postgres.py          # Database loader
â”‚       â””â”€â”€ upsert_logic.py           # Upsert operations
â”‚
â”œâ”€â”€ ğŸ“ tests/                     # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # Pytest fixtures
â”‚   â”œâ”€â”€ test_parsing.py           # Parsing tests
â”‚   â””â”€â”€ test_matching.py          # Matching tests
â”‚
â”œâ”€â”€ ğŸ“ dbt/                       # dbt transformations
â”‚   â”œâ”€â”€ dbt_project.yml           # dbt configuration
â”‚   â”œâ”€â”€ profiles.yml              # Connection profiles
â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # stg_web_companies, stg_abr_entities
â”‚   â”‚   â”œâ”€â”€ intermediate/         # int_matched_companies
â”‚   â”‚   â””â”€â”€ marts/                # dim_companies, fct_match_statistics
â”‚   â””â”€â”€ ğŸ“ tests/                 # Data quality tests
â”‚
â””â”€â”€ ğŸ“ data/                      # Data directories (gitignored)
    â”œâ”€â”€ raw/                      # Downloaded files
    â”œâ”€â”€ processed/                # Cleaned data
    â””â”€â”€ output/                   # Final results
```

---

## ğŸ“Š Database Schema

### Tables Overview

| Table | Description | Primary Key |
|-------|-------------|-------------|
| `web_companies` | Common Crawl extracted data | `id` (auto) |
| `abr_entities` | ABR registry data | `abn` |
| `entity_match_results` | Match pairs with scores | `id` (auto) |
| `unified_companies` | Golden merged records | `abn` |

### `web_companies` (from Common Crawl)

```sql
CREATE TABLE web_companies (
    id              BIGSERIAL PRIMARY KEY,
    url             TEXT NOT NULL,
    domain          TEXT,
    company_name    TEXT,
    normalized_name TEXT,
    industry        TEXT,
    raw_text        TEXT,
    block_key       VARCHAR(10),
    created_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### `abr_entities` (from ABR XML)

```sql
CREATE TABLE abr_entities (
    abn             VARCHAR(11) PRIMARY KEY,
    entity_name     TEXT NOT NULL,
    normalized_name TEXT,
    entity_type     VARCHAR(50),    -- PRV, PUB, IND, TRT
    entity_status   VARCHAR(20),    -- Active, Cancelled
    state           VARCHAR(10),    -- NSW, VIC, QLD, etc.
    postcode        VARCHAR(10),
    start_date      DATE,
    block_key       VARCHAR(10)
);
```

### `entity_match_results` (Matching Output)

```sql
CREATE TABLE entity_match_results (
    id              BIGSERIAL PRIMARY KEY,
    crawl_name      TEXT,
    crawl_url       TEXT,
    abr_name        TEXT,
    abn             VARCHAR(11) REFERENCES abr_entities(abn),
    fuzzy_score     DECIMAL(5,4),   -- 0.0000 - 1.0000
    llm_score       DECIMAL(5,4),
    final_score     DECIMAL(5,4),
    match_method    VARCHAR(20),    -- 'fuzzy', 'llm', 'hybrid'
    UNIQUE(crawl_url, abn)
);
```

### `unified_companies` (Golden Record)

```sql
CREATE TABLE unified_companies (
    abn              VARCHAR(11) PRIMARY KEY,
    canonical_name   TEXT NOT NULL,
    trading_name     TEXT,
    url              TEXT,
    domain           TEXT,
    industry         TEXT,
    entity_type      VARCHAR(50),
    entity_status    VARCHAR(20),
    state            VARCHAR(10),
    postcode         VARCHAR(10),
    start_date       DATE,
    source           VARCHAR(20),    -- 'ABR', 'CC', 'MERGED'
    confidence_score DECIMAL(5,4)
);
```

---

## ğŸ”§ Entity Matching Algorithm

### Three-Stage Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: BLOCKING                                               â”‚
â”‚                                                                 â”‚
â”‚   Purpose: Reduce comparison space                              â”‚
â”‚   Method:  Group records by first 4 chars of normalized name   â”‚
â”‚   Result:  200K Ã— 3M â†’ ~10K pairs per block                    â”‚
â”‚                                                                 â”‚
â”‚   Example:                                                      â”‚
â”‚   "ACME Corp" â†’ block_key = "acme"                             â”‚
â”‚   "ACME Holdings Pty Ltd" â†’ block_key = "acme"                 â”‚
â”‚   â†’ These will be compared                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: FUZZY MATCHING                                         â”‚
â”‚                                                                 â”‚
â”‚   Algorithm: RapidFuzz token_sort_ratio                        â”‚
â”‚   Threshold: 0.75 minimum for match                            â”‚
â”‚                                                                 â”‚
â”‚   Example:                                                      â”‚
â”‚   "ACME CORP" vs "ACME CORPORATION PTY LTD"                    â”‚
â”‚   â†’ Fuzzy Score: 0.82 âœ“                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: LLM VERIFICATION (Optional)                            â”‚
â”‚                                                                 â”‚
â”‚   Trigger: Fuzzy score between 0.60 - 0.85                     â”‚
â”‚   Model:   GPT-4o-mini                                          â”‚
â”‚   Cost:    ~$0.001 per comparison                              â”‚
â”‚                                                                 â”‚
â”‚   Prompt:                                                       â”‚
â”‚   "Are these the same company?                                  â”‚
â”‚    1. ACME Digital Services                                     â”‚
â”‚    2. ACME DIGITAL SERVICES PTY LTD                            â”‚
â”‚    Return: {match: true/false, score: 0-1, reason: '...'}"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINAL SCORE CALCULATION                                         â”‚
â”‚                                                                 â”‚
â”‚   Formula: final_score = 0.7 Ã— fuzzy + 0.3 Ã— llm               â”‚
â”‚   Decision: Match if final_score â‰¥ 0.75                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ğŸ™ Acknowledgments

- [Common Crawl](https://commoncrawl.org/) - Web archive data
- [data.gov.au](https://data.gov.au/) - ABR bulk data
- [RapidFuzz](https://github.com/maxbachmann/RapidFuzz) - Fast fuzzy matching
- [OpenAI](https://openai.com/) - GPT models for semantic matching
